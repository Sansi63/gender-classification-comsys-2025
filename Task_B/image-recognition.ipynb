{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a0754d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:41:27.010935Z",
     "iopub.status.busy": "2025-07-01T13:41:27.010734Z",
     "iopub.status.idle": "2025-07-01T13:41:34.645331Z",
     "shell.execute_reply": "2025-07-01T13:41:34.644501Z"
    },
    "papermill": {
     "duration": 7.639346,
     "end_time": "2025-07-01T13:41:34.646509",
     "exception": false,
     "start_time": "2025-07-01T13:41:27.007163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepface\r\n",
      "  Downloading deepface-0.0.93-py3-none-any.whl.metadata (30 kB)\r\n",
      "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.32.3)\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (1.26.4)\r\n",
      "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.2.3)\r\n",
      "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (5.2.0)\r\n",
      "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.67.1)\r\n",
      "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (11.1.0)\r\n",
      "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.11.0.86)\r\n",
      "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.18.0)\r\n",
      "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.8.0)\r\n",
      "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.1.0)\r\n",
      "Collecting flask-cors>=4.0.1 (from deepface)\r\n",
      "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\r\n",
      "Collecting mtcnn>=0.1.0 (from deepface)\r\n",
      "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Collecting retina-face>=0.0.1 (from deepface)\r\n",
      "  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting fire>=0.4.0 (from deepface)\r\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting gunicorn>=20.1.0 (from deepface)\r\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\r\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.4.0->deepface) (3.0.1)\r\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.3)\r\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.6)\r\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\r\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (8.1.8)\r\n",
      "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (1.9.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (4.13.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (3.18.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gunicorn>=20.1.0->deepface) (25.0)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (1.4.0)\r\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (14.0.0)\r\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.0.8)\r\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (3.13.0)\r\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.14.1)\r\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\r\n",
      "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn>=0.1.0->deepface) (1.5.0)\r\n",
      "Collecting lz4>=4.3.3 (from mtcnn>=0.1.0->deepface)\r\n",
      "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->deepface) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->deepface) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->deepface) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->deepface) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->deepface) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->deepface) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2025.4.26)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (25.2.10)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (3.20.3)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (75.2.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (4.13.2)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.2)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.72.0rc1)\r\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (2.18.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=1.1.2->deepface) (3.0.2)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (3.7)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (0.7.2)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.6)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->deepface) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->deepface) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->deepface) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->deepface) (2024.2.0)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (2.19.1)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->deepface) (2024.2.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\r\n",
      "Downloading deepface-0.0.93-py3-none-any.whl (108 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\r\n",
      "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading retina_face-0.0.17-py3-none-any.whl (25 kB)\r\n",
      "Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: fire\r\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=073c477c19602375c55a2c7bc5945c688221371cc1254c9c99e8cbe9d168d687\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\r\n",
      "Successfully built fire\r\n",
      "Installing collected packages: lz4, gunicorn, fire, mtcnn, flask-cors, retina-face, deepface\r\n",
      "Successfully installed deepface-0.0.93 fire-0.7.0 flask-cors-6.0.1 gunicorn-23.0.0 lz4-4.4.4 mtcnn-1.0.0 retina-face-0.0.17\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b319047",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-01T13:41:34.655310Z",
     "iopub.status.busy": "2025-07-01T13:41:34.655048Z",
     "iopub.status.idle": "2025-07-01T13:41:50.211237Z",
     "shell.execute_reply": "2025-07-01T13:41:50.210443Z"
    },
    "papermill": {
     "duration": 15.562078,
     "end_time": "2025-07-01T13:41:50.212707",
     "exception": false,
     "start_time": "2025-07-01T13:41:34.650629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 13:41:37.739380: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751377297.932683      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751377297.991076      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-07-01 13:41:49 - Directory /root/.deepface has been created\n",
      "25-07-01 13:41:49 - Directory /root/.deepface/weights has been created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c503e1d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:41:50.221239Z",
     "iopub.status.busy": "2025-07-01T13:41:50.220811Z",
     "iopub.status.idle": "2025-07-01T13:41:51.679889Z",
     "shell.execute_reply": "2025-07-01T13:41:51.679294Z"
    },
    "papermill": {
     "duration": 1.464623,
     "end_time": "2025-07-01T13:41:51.681258",
     "exception": false,
     "start_time": "2025-07-01T13:41:50.216635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define paths (adjust according to your dataset)\n",
    "TRAIN_DATASET_PATH = '/kaggle/input/fully-train/train'\n",
    "VAL_DATASET_PATH = \"/kaggle/input/validation/val\"\n",
    "IDENTITY_FOLDERS_TRAIN = [f for f in os.listdir(TRAIN_DATASET_PATH) if os.path.isdir(os.path.join(TRAIN_DATASET_PATH, f))]\n",
    "IDENTITY_FOLDERS_VAL = [f for f in os.listdir(VAL_DATASET_PATH) if os.path.isdir(os.path.join(VAL_DATASET_PATH, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccfdfc77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:41:51.689707Z",
     "iopub.status.busy": "2025-07-01T13:41:51.689318Z",
     "iopub.status.idle": "2025-07-01T13:41:51.700339Z",
     "shell.execute_reply": "2025-07-01T13:41:51.699812Z"
    },
    "papermill": {
     "duration": 0.016299,
     "end_time": "2025-07-01T13:41:51.701390",
     "exception": false,
     "start_time": "2025-07-01T13:41:51.685091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to load clear and distorted images and extract embeddings\n",
    "def load_and_embed_images(folder_path):\n",
    "    clear_embeddings = []\n",
    "    distorted_embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    # Assuming VGG-Face produces 4096-dimensional embeddings\n",
    "    EMBEDDING_DIM = 4096 \n",
    "\n",
    "    for identity in os.listdir(folder_path):\n",
    "        identity_full_path = os.path.join(folder_path, identity)\n",
    "        if os.path.isdir(identity_full_path):\n",
    "            # Load clear reference image (assuming one clear image per folder)\n",
    "            clear_images = [f for f in os.listdir(identity_full_path) if f.lower().endswith(('.jpg', '.jpeg', '.png')) and not os.path.isdir(os.path.join(identity_full_path, f))]\n",
    "            \n",
    "            # Process clear images\n",
    "            if clear_images:\n",
    "                clear_img_path = os.path.join(identity_full_path, clear_images[0])\n",
    "                try:\n",
    "                    clear_embedding_dicts = DeepFace.represent(clear_img_path, model_name=\"VGG-Face\", enforce_detection=False)\n",
    "                    if clear_embedding_dicts:\n",
    "                        clear_embedding = clear_embedding_dicts[0]['embedding']\n",
    "                        clear_embeddings.append(clear_embedding)\n",
    "                        labels.append(identity) # Label for the clear image\n",
    "                    else:\n",
    "                        print(f\"No embeddings found for clear image {clear_img_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing clear image {clear_img_path}: {e}\")\n",
    "            else:\n",
    "                print(f\"No clear images found in {identity_full_path}\")\n",
    "\n",
    "            # Load distorted images from the nested distorted folder\n",
    "            distorted_folder = os.path.join(identity_full_path, \"distortion\")\n",
    "            if os.path.isdir(distorted_folder):\n",
    "                for dist_img_name in os.listdir(distorted_folder):\n",
    "                    dist_img_path = os.path.join(distorted_folder, dist_img_name)\n",
    "                    if dist_img_name.lower().endswith(('.jpg', '.jpeg', '.png')): # Ensure it's an image file\n",
    "                        try:\n",
    "                            dist_embedding_dicts = DeepFace.represent(dist_img_path, model_name=\"VGG-Face\", enforce_detection=False)\n",
    "                            if dist_embedding_dicts:\n",
    "                                dist_embedding = dist_embedding_dicts[0]['embedding']\n",
    "                                distorted_embeddings.append(dist_embedding)\n",
    "                                labels.append(identity) # Label for the distorted image\n",
    "                            else:\n",
    "                                print(f\"No embeddings found for distorted image {dist_img_path}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing distorted image {dist_img_path}: {e}\")\n",
    "            else:\n",
    "                print(f\"Distorted folder not found for identity {identity} at {distorted_folder}\")\n",
    "    \n",
    "    # Convert to numpy arrays. Ensure they are 2D and have the correct embedding dimension.\n",
    "    # If empty, create an empty array with the correct second dimension for concatenation.\n",
    "    if not clear_embeddings:\n",
    "        clear_embeddings_np = np.empty((0, EMBEDDING_DIM))\n",
    "    else:\n",
    "        clear_embeddings_np = np.array(clear_embeddings)\n",
    "        if clear_embeddings_np.ndim == 1: # If only one embedding was found, it might be 1D\n",
    "            clear_embeddings_np = clear_embeddings_np.reshape(1, -1)\n",
    "        # Ensure the second dimension matches EMBEDDING_DIM, if not, there's an issue with DeepFace output\n",
    "        if clear_embeddings_np.shape[1] != EMBEDDING_DIM:\n",
    "            print(f\"Warning: Clear embeddings have unexpected dimension {clear_embeddings_np.shape[1]}, expected {EMBEDDING_DIM}\")\n",
    "            # Handle this case, e.g., by filtering or padding, or raising an error.\n",
    "            # For now, we'll proceed, but this might lead to further errors if not handled.\n",
    "\n",
    "    if not distorted_embeddings:\n",
    "        distorted_embeddings_np = np.empty((0, EMBEDDING_DIM))\n",
    "    else:\n",
    "        distorted_embeddings_np = np.array(distorted_embeddings)\n",
    "        if distorted_embeddings_np.ndim == 1: # If only one embedding was found, it might be 1D\n",
    "            distorted_embeddings_np = distorted_embeddings_np.reshape(1, -1)\n",
    "        # Ensure the second dimension matches EMBEDDING_DIM\n",
    "        if distorted_embeddings_np.shape[1] != EMBEDDING_DIM:\n",
    "            print(f\"Warning: Distorted embeddings have unexpected dimension {distorted_embeddings_np.shape[1]}, expected {EMBEDDING_DIM}\")\n",
    "            \n",
    "    return clear_embeddings_np, distorted_embeddings_np, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26399a4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:41:51.708902Z",
     "iopub.status.busy": "2025-07-01T13:41:51.708694Z",
     "iopub.status.idle": "2025-07-01T13:41:51.713210Z",
     "shell.execute_reply": "2025-07-01T13:41:51.712559Z"
    },
    "papermill": {
     "duration": 0.009476,
     "end_time": "2025-07-01T13:41:51.714338",
     "exception": false,
     "start_time": "2025-07-01T13:41:51.704862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Siamese Network for learning similarity\n",
    "def create_siamese_network(input_shape):\n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "    \n",
    "    # Shared layers for feature extraction\n",
    "    # Define the embedding network once and reuse it for both inputs\n",
    "    embedding_network = tf.keras.Sequential([\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dense(64, activation=\"relu\")\n",
    "    ])\n",
    "    \n",
    "    processed_a = embedding_network(input_a)\n",
    "    processed_b = embedding_network(input_b)\n",
    "    \n",
    "    # Calculate Euclidean distance\n",
    "    distance = Lambda(lambda tensors: tf.reduce_sum(tf.square(tensors[0] - tensors[1]), axis=1, keepdims=True))([processed_a, processed_b])\n",
    "    \n",
    "    siamese_model = Model(inputs=[input_a, input_b], outputs=distance)\n",
    "    return siamese_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fac8f0d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:41:51.721899Z",
     "iopub.status.busy": "2025-07-01T13:41:51.721720Z",
     "iopub.status.idle": "2025-07-01T13:41:51.725468Z",
     "shell.execute_reply": "2025-07-01T13:41:51.724830Z"
    },
    "papermill": {
     "duration": 0.008827,
     "end_time": "2025-07-01T13:41:51.726635",
     "exception": false,
     "start_time": "2025-07-01T13:41:51.717808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1.0  # Define the margin hyperparameter\n",
    "    y_true = tf.cast(y_true, tf.float32)  # Ensure y_true is float32\n",
    "    square_pred = tf.square(y_pred)\n",
    "    margin_square = tf.square(tf.maximum(margin - y_pred, 0.0))\n",
    "    return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39dffce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:41:51.734343Z",
     "iopub.status.busy": "2025-07-01T13:41:51.734144Z",
     "iopub.status.idle": "2025-07-01T13:41:51.741924Z",
     "shell.execute_reply": "2025-07-01T13:41:51.741430Z"
    },
    "papermill": {
     "duration": 0.012843,
     "end_time": "2025-07-01T13:41:51.742946",
     "exception": false,
     "start_time": "2025-07-01T13:41:51.730103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate pairs for training\n",
    "def generate_pairs(clear_embeddings, distorted_embeddings, labels):\n",
    "    pairs = []\n",
    "    pair_labels = []\n",
    "\n",
    "    # It's crucial that all_embeddings and all_labels are correctly aligned.\n",
    "    # If labels are associated with individual embeddings, then concatenate them.\n",
    "    # Assuming labels list contains an entry for each clear and each distorted embedding.\n",
    "    # If labels are per identity, then this needs to be adjusted.\n",
    "    # For simplicity, let's assume labels are per identity and we need to map them to embeddings.\n",
    "    \n",
    "    # A more robust way to generate pairs would be to iterate through identities\n",
    "    # and then pick clear/distorted images from that identity for positive pairs,\n",
    "    # and images from other identities for negative pairs.\n",
    "\n",
    "    # Let's refine the pair generation based on the assumption that `labels`\n",
    "    # contains the identity for each corresponding embedding in `clear_embeddings`\n",
    "    # and `distorted_embeddings`.\n",
    "    \n",
    "    # Create a mapping from identity to list of embeddings for that identity\n",
    "    identity_to_embeddings = {}\n",
    "    for i, emb in enumerate(clear_embeddings):\n",
    "        identity = labels[i] # Assuming labels are aligned with clear_embeddings first\n",
    "        if identity not in identity_to_embeddings:\n",
    "            identity_to_embeddings[identity] = []\n",
    "        identity_to_embeddings[identity].append(emb)\n",
    "    \n",
    "    # Adjusting for distorted embeddings and their labels.\n",
    "    # The original `load_and_embed_images` appends labels for both clear and distorted images.\n",
    "    # So, `labels` will contain `len(clear_embeddings) + len(distorted_embeddings)` entries.\n",
    "    # We need to ensure `all_embeddings` and `all_labels` are correctly formed.\n",
    "\n",
    "    # Let's re-think `all_embeddings` and `all_labels` based on the `load_and_embed_images` output.\n",
    "    # `load_and_embed_images` appends `identity` to `labels` for each embedding it successfully extracts.\n",
    "    # So, `labels` will contain the identity for each embedding in `clear_embeddings` followed by `distorted_embeddings`.\n",
    "    \n",
    "    # This means `labels` is already a combined list of labels for all embeddings.\n",
    "    # We need to ensure `all_embeddings` is also correctly combined.\n",
    "    \n",
    "    # Check if clear_embeddings or distorted_embeddings are empty before concatenation\n",
    "    if clear_embeddings.size == 0 and distorted_embeddings.size == 0:\n",
    "        print(\"Warning: No embeddings available to generate pairs.\")\n",
    "        return np.array([]).reshape(0, 2, EMBEDDING_DIM), np.array([]) # Return empty arrays with correct shape\n",
    "\n",
    "    # Ensure concatenation works even if one is empty but the other is not\n",
    "    if clear_embeddings.size == 0:\n",
    "        all_embeddings = distorted_embeddings\n",
    "    elif distorted_embeddings.size == 0:\n",
    "        all_embeddings = clear_embeddings\n",
    "    else:\n",
    "        all_embeddings = np.concatenate([clear_embeddings, distorted_embeddings], axis=0)\n",
    "    \n",
    "    # The `labels` list from `load_and_embed_images` already contains labels for all extracted embeddings.\n",
    "    # So, `all_labels` is simply the `labels` list.\n",
    "    all_labels = labels \n",
    "\n",
    "    # Create a dictionary to group embeddings by identity\n",
    "    embeddings_by_identity = {}\n",
    "    for i, emb in enumerate(all_embeddings):\n",
    "        current_label = all_labels[i]\n",
    "        if current_label not in embeddings_by_identity:\n",
    "            embeddings_by_identity[current_label] = []\n",
    "        embeddings_by_identity[current_label].append(emb)\n",
    "\n",
    "    # Generate pairs\n",
    "    for i, (emb1, label1) in enumerate(zip(all_embeddings, all_labels)):\n",
    "        # Positive pair (same identity)\n",
    "        # Pick another embedding from the same identity\n",
    "        same_identity_embeddings = [e for e in embeddings_by_identity[label1] if not np.array_equal(e, emb1)]\n",
    "        if same_identity_embeddings:\n",
    "            pos_emb = same_identity_embeddings[np.random.choice(len(same_identity_embeddings))]\n",
    "            pairs.append([emb1, pos_emb])\n",
    "            pair_labels.append(1)\n",
    "        \n",
    "        # Negative pair (different identity)\n",
    "        # Pick an embedding from a different identity\n",
    "        other_identities = [l for l in embeddings_by_identity if l != label1]\n",
    "        if other_identities:\n",
    "            neg_label = np.random.choice(other_identities)\n",
    "            neg_emb = embeddings_by_identity[neg_label][np.random.choice(len(embeddings_by_identity[neg_label]))]\n",
    "            pairs.append([emb1, neg_emb])\n",
    "            pair_labels.append(0)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    if not pairs:\n",
    "        # If no pairs could be generated, return empty arrays with the expected shape\n",
    "        return np.array([]).reshape(0, 2, EMBEDDING_DIM), np.array([])\n",
    "    \n",
    "    return np.array(pairs), np.array(pair_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09cb62ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:41:51.750451Z",
     "iopub.status.busy": "2025-07-01T13:41:51.750247Z",
     "iopub.status.idle": "2025-07-01T13:41:51.756673Z",
     "shell.execute_reply": "2025-07-01T13:41:51.756140Z"
    },
    "papermill": {
     "duration": 0.011471,
     "end_time": "2025-07-01T13:41:51.757715",
     "exception": false,
     "start_time": "2025-07-01T13:41:51.746244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Load training data\n",
    "    print(\"Loading training data and generating embeddings...\")\n",
    "    clear_train_embeddings, distorted_train_embeddings, train_labels = load_and_embed_images(TRAIN_DATASET_PATH)\n",
    "    print(f\"Train: Clear embeddings shape: {clear_train_embeddings.shape}, Distorted embeddings shape: {distorted_train_embeddings.shape}\")\n",
    "    \n",
    "    # Load validation data\n",
    "    print(\"Loading validation data and generating embeddings...\")\n",
    "    clear_val_embeddings, distorted_val_embeddings, val_labels = load_and_embed_images(VAL_DATASET_PATH)\n",
    "    print(f\"Validation: Clear embeddings shape: {clear_val_embeddings.shape}, Distorted embeddings shape: {distorted_val_embeddings.shape}\")\n",
    "\n",
    "    # Generate pairs for training and validation\n",
    "    print(\"Generating training pairs...\")\n",
    "    train_pairs, train_pair_labels = generate_pairs(clear_train_embeddings, distorted_train_embeddings, train_labels)\n",
    "    print(f\"Generated {len(train_pairs)} training pairs.\")\n",
    "\n",
    "    print(\"Generating validation pairs...\")\n",
    "    val_pairs, val_pair_labels = generate_pairs(clear_val_embeddings, distorted_val_embeddings, val_labels)\n",
    "    print(f\"Generated {len(val_pairs)} validation pairs.\")\n",
    "\n",
    "    # Check if there are enough pairs to train\n",
    "    if len(train_pairs) == 0:\n",
    "        print(\"Error: No training pairs generated. Cannot train the model.\")\n",
    "        return\n",
    "    if len(val_pairs) == 0:\n",
    "        print(\"Warning: No validation pairs generated. Model will be trained without validation.\")\n",
    "        validation_data = None\n",
    "    else:\n",
    "        validation_data = ([val_pairs[:, 0], val_pairs[:, 1]], val_pair_labels)\n",
    "\n",
    "    # Create and compile Siamese Network\n",
    "    # The input_shape should be the dimension of a single embedding (e.g., 4096,)\n",
    "    # We need to ensure clear_train_embeddings is not empty to get its shape.\n",
    "    if clear_train_embeddings.shape[0] > 0:\n",
    "        input_shape = clear_train_embeddings.shape[1:] # Should be (4096,)\n",
    "    elif distorted_train_embeddings.shape[0] > 0:\n",
    "        input_shape = distorted_train_embeddings.shape[1:] # Should be (4096,)\n",
    "    else:\n",
    "        print(\"Error: No embeddings found to determine input shape for the Siamese network.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Siamese Network input shape: {input_shape}\")\n",
    "    siamese_model = create_siamese_network(input_shape)\n",
    "    siamese_model.compile(loss=contrastive_loss, optimizer=Adam(learning_rate=0.0001))\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training the Siamese model...\")\n",
    "    siamese_model.fit(\n",
    "        [train_pairs[:, 0], train_pairs[:, 1]], train_pair_labels,\n",
    "        validation_data=validation_data,\n",
    "        epochs=20, batch_size=32, verbose=1\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    siamese_model.save(\"siamese_face_matching.h5\")\n",
    "    print(\"Model saved as siamese_face_matching.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e60af595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:41:51.765048Z",
     "iopub.status.busy": "2025-07-01T13:41:51.764854Z",
     "iopub.status.idle": "2025-07-01T15:03:03.820561Z",
     "shell.execute_reply": "2025-07-01T15:03:03.819711Z"
    },
    "papermill": {
     "duration": 4872.060712,
     "end_time": "2025-07-01T15:03:03.821812",
     "exception": false,
     "start_time": "2025-07-01T13:41:51.761100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data and generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751377313.007098      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-07-01 13:41:53 - vgg_face_weights.h5 will be downloaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/serengil/deepface_models/releases/download/v1.0/vgg_face_weights.h5\n",
      "To: /root/.deepface/weights/vgg_face_weights.h5\n",
      "100%|██████████| 580M/580M [00:06<00:00, 87.7MB/s]\n",
      "I0000 00:00:1751377323.385443      19 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Clear embeddings shape: (877, 4096), Distorted embeddings shape: (13482, 4096)\n",
      "Loading validation data and generating embeddings...\n",
      "Validation: Clear embeddings shape: (250, 4096), Distorted embeddings shape: (2954, 4096)\n",
      "Generating training pairs...\n",
      "Generated 28718 training pairs.\n",
      "Generating validation pairs...\n",
      "Generated 6408 validation pairs.\n",
      "Siamese Network input shape: (4096,)\n",
      "Training the Siamese model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751382127.850301      74 service.cc:148] XLA service 0x7a4608918930 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1751382127.850977      74 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1751382128.110980      74 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898/898 [==============================] - 7s 4ms/step - loss: 0.2704 - val_loss: 0.2508\n",
      "Epoch 2/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.2187 - val_loss: 0.2449\n",
      "Epoch 3/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.2000 - val_loss: 0.2424\n",
      "Epoch 4/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1861 - val_loss: 0.2423\n",
      "Epoch 5/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1749 - val_loss: 0.2418\n",
      "Epoch 6/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1655 - val_loss: 0.2426\n",
      "Epoch 7/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1571 - val_loss: 0.2449\n",
      "Epoch 8/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1499 - val_loss: 0.2460\n",
      "Epoch 9/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1436 - val_loss: 0.2472\n",
      "Epoch 10/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1380 - val_loss: 0.2477\n",
      "Epoch 11/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1329 - val_loss: 0.2486\n",
      "Epoch 12/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1282 - val_loss: 0.2500\n",
      "Epoch 13/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1238 - val_loss: 0.2527\n",
      "Epoch 14/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1195 - val_loss: 0.2560\n",
      "Epoch 15/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1158 - val_loss: 0.2541\n",
      "Epoch 16/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1126 - val_loss: 0.2553\n",
      "Epoch 17/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1090 - val_loss: 0.2577\n",
      "Epoch 18/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1061 - val_loss: 0.2547\n",
      "Epoch 19/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1032 - val_loss: 0.2597\n",
      "Epoch 20/20\n",
      "898/898 [==============================] - 3s 3ms/step - loss: 0.1004 - val_loss: 0.2612\n",
      "Model saved as siamese_face_matching.h5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8e599",
   "metadata": {
    "papermill": {
     "duration": 0.044657,
     "end_time": "2025-07-01T15:03:03.910804",
     "exception": false,
     "start_time": "2025-07-01T15:03:03.866147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee618f",
   "metadata": {
    "papermill": {
     "duration": 0.043172,
     "end_time": "2025-07-01T15:03:03.997999",
     "exception": false,
     "start_time": "2025-07-01T15:03:03.954827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7766527,
     "sourceId": 12321276,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7766644,
     "sourceId": 12321445,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7766670,
     "sourceId": 12321485,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4904.575942,
   "end_time": "2025-07-01T15:03:07.656715",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-01T13:41:23.080773",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
